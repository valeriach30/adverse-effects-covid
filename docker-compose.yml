volumes:
  metadata_data: {}
  middle_var: {}
  historical_var: {}
  broker_var: {}
  coordinator_var: {}
  router_var: {}
  druid_shared: {}
  shared_data: {} # Volumen compartido para datos entre servicios

services:
  # --- Inicialización de volumen compartido ---
  volume-init:
    image: busybox:latest
    container_name: volume_init_vaers
    volumes:
      - shared_data:/opt/shared_data
    command: >
      sh -c "
      mkdir -p /opt/shared_data/vaers_results /opt/shared_data/druid_ingestion_specs &&
      chmod -R 777 /opt/shared_data &&
      echo '✅ Volumen compartido inicializado con permisos correctos'
      "
    restart: "no"

  # --- Postgres (Airflow + Superset + Druid metadata) ---
  postgres:
    container_name: postgres
    image: postgres:latest
    ports:
      - "5432:5432"
    volumes:
      - metadata_data:/var/lib/postgresql/data
      - ./init-db.sh:/docker-entrypoint-initdb.d/init-db.sh
    environment:
      - POSTGRES_PASSWORD=FoolishPassword
      - POSTGRES_USER=postgres
      - POSTGRES_DB=postgres
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
  # --- Airflow Initialization ---
  airflow-init:
    image: "apache/airflow:2.9.0"
    container_name: airflow_init_vaers
    depends_on:
      postgres:
        condition: service_healthy
    volumes:
      - ./dags:/opt/airflow/dags
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__WEBSERVER__SECRET_KEY: unaclavesecretaparaelproyecto
    entrypoint: /bin/bash
    command: -c "airflow db migrate && airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin"
    restart: "no"
  # --- Airflow Webserver ---
  airflow-webserver:
    image: "apache/airflow:2.9.0"
    container_name: airflow_webserver_vaers
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      volume-init:
        condition: service_completed_successfully
    volumes:
      - ./dags:/opt/airflow/dags
      - ./spark:/opt/airflow/spark
      - ./superset:/opt/airflow/superset
      - ./data:/opt/airflow/data
      - shared_data:/opt/shared_data # Datos compartidos entre servicios
    ports:
      - "8080:8080"
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__WEBSERVER__SECRET_KEY: unaclavesecretaparaelproyecto
    command: "webserver"
    restart: always
  # --- Airflow Scheduler ---
  airflow-scheduler:
    image: "apache/airflow:2.9.0"
    container_name: airflow_scheduler_vaers
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      volume-init:
        condition: service_completed_successfully
    volumes:
      - ./dags:/opt/airflow/dags
      - ./spark:/opt/airflow/spark
      - ./superset:/opt/airflow/superset
      - ./data:/opt/airflow/data
      - shared_data:/opt/shared_data # Datos compartidos entre servicios
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__WEBSERVER__SECRET_KEY: unaclavesecretaparaelproyecto
    command: "scheduler"
    restart: always

  # --- Spark Master ---
  spark-master:
    image: "apache/spark:3.5.3"
    container_name: spark_master_vaers
    hostname: spark-master
    depends_on:
      volume-init:
        condition: service_completed_successfully
    ports:
      - "8084:8080" # UI Spark Master
      - "7077:7077" # Puerto Spark Master
    volumes:
      - ./data:/opt/spark/data # Datos CSV de entrada
      - shared_data:/opt/shared_data # Datos procesados de salida
    environment:
      SPARK_MASTER_HOST: spark-master
    command: >
      /bin/bash -c "/opt/spark/sbin/start-master.sh && tail -f /dev/null"
    restart: always

  # --- Spark Worker ---
  spark-worker:
    image: "apache/spark:3.5.3"
    container_name: spark_worker_vaers
    depends_on:
      - spark-master
      - volume-init
    volumes:
      - ./data:/opt/spark/data # Datos CSV de entrada
      - shared_data:/opt/shared_data # Datos procesados de salida
    environment:
      SPARK_MASTER: spark://spark-master:7077
      SPARK_WORKER_DIR: /opt/spark/work
      SPARK_LOG_DIR: /opt/spark/logs
    command: >
      /bin/bash -c "/opt/spark/sbin/start-worker.sh spark://spark-master:7077 && tail -f /dev/null"
    restart: always

  # Need 3.5 or later for container nodes
  zookeeper:
    container_name: zookeeper
    image: zookeeper:3.5.10
    ports:
      - "2181:2181"
    environment:
      - ZOO_MY_ID=1

  coordinator:
    image: apache/druid:34.0.0
    container_name: coordinator
    volumes:
      - druid_shared:/opt/shared
      - coordinator_var:/opt/druid/var
      - shared_data:/opt/shared_data
    depends_on:
      - zookeeper
      - postgres
    ports:
      - "8081:8081"
    command:
      - coordinator
    env_file:
      - environment

  broker:
    image: apache/druid:34.0.0
    container_name: broker
    volumes:
      - broker_var:/opt/druid/var
      - shared_data:/opt/shared_data # Acceso a datos compartidos
    depends_on:
      - zookeeper
      - postgres
      - coordinator
      - volume-init
    ports:
      - "8082:8082"
    command:
      - broker
    env_file:
      - environment

  historical:
    image: apache/druid:34.0.0
    container_name: historical
    volumes:
      - druid_shared:/opt/shared
      - historical_var:/opt/druid/var
      - shared_data:/opt/shared_data # Acceso a datos compartidos
    depends_on:
      - volume-init
      - zookeeper
      - postgres
      - coordinator
    ports:
      - "8083:8083"
    command:
      - historical
    env_file:
      - environment

  middlemanager:
    image: apache/druid:34.0.0
    container_name: middlemanager
    volumes:
      - druid_shared:/opt/shared
      - middle_var:/opt/druid/var
      - shared_data:/opt/shared_data # Acceso a datos compartidos
    depends_on:
      - volume-init
      - zookeeper
      - postgres
      - coordinator
    ports:
      - "8091:8091"
      - "8100-8105:8100-8105"
    command:
      - middleManager
    env_file:
      - environment

  router:
    image: apache/druid:34.0.0
    container_name: router
    volumes:
      - router_var:/opt/druid/var
    depends_on:
      - zookeeper
      - postgres
      - coordinator
    ports:
      - "8888:8888"
    command:
      - router
    env_file:
      - environment

  # --- Superset ---
  superset:
    image: apache/superset:4.0.0
    container_name: superset
    depends_on:
      postgres:
        condition: service_healthy
      broker:
        # Espera a que Druid esté disponible
        condition: service_started
    ports:
      - "8088:8088" # UI Superset
    environment:
      SUPERSET_SECRET_KEY: "unaclavesecretaparaSuperset"
      # Configuración para metadata de Superset en PostgreSQL
      SUPERSET_CONFIG_PATH: /app/superset_config.py
    volumes:
      - ./superset_config.py:/app/superset_config.py:ro # Archivo de configuración personalizado
    command: >
      /bin/bash -c "
        sleep 30 &&
        superset db upgrade &&
        superset fab create-admin --username admin --firstname Admin --lastname User --email admin@example.com --password admin &&
        superset init &&
        gunicorn --bind 0.0.0.0:8088 'superset.app:create_app()'
      "
    restart: always
